# GitHub Copilot Context Instructions

## Project Understanding for AI Assistants

### ğŸ¯ Project Purpose
This is a **LoRA (Low-Rank Adaptation) fine-tuning project** for creating personalized AI models based on Slack conversation data. The goal is to train a model that can generate responses matching a specific user's communication style and personality.

### ğŸ—ï¸ Architecture Overview

#### Core Components
1. **Data Processing Pipeline** (`slack_data_processor.py`)
   - Converts Slack export JSON files to training datasets
   - Implements industry-standard Alpaca format
   - Handles conversation threading and context preservation
   - User selection and activity analysis

2. **Training Pipeline** (`LoRA-slack-persona-local.ipynb`)
   - LoRA fine-tuning using HuggingFace transformers
   - Optimized for RTX A4000 GPU (16GB VRAM)
   - Memory-efficient quantization (4-bit)
   - Automatic dataset detection and loading

3. **Environment Management** (`setup_env.py`)
   - Modern package management using `uv` (not pip)
   - Cross-platform environment verification
   - GPU detection and PyTorch CUDA validation

### ğŸ”§ Technical Stack

#### Package Manager: `uv` (Important!)
- **NOT using pip or conda**
- Commands: `uv pip install`, `uv venv`, etc.
- 10-100x faster than pip
- Better dependency resolution

#### ML/AI Libraries
```python
# Core
torch>=2.0.0 (CUDA-enabled)
transformers>=4.35.0
datasets>=2.14.0
peft>=0.6.0 (LoRA implementation)
accelerate>=0.24.0
bitsandbytes>=0.41.0 (quantization)
trl>=0.7.0 (training utilities)

# Data Science
pandas, numpy, matplotlib, seaborn, scikit-learn

# Development
jupyter, ipykernel, ipywidgets
```

#### Model Configuration
```python
# Base Model
model_id = "stabilityai/japanese-stablelm-instruct-gamma-7b"

# LoRA Config
LoraConfig(
    r=16,                    # Low rank for efficiency
    lora_alpha=32,          # Scaling factor
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                   "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

# Training Args (RTX A4000 optimized)
TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    learning_rate=2e-4,
    bf16=True,              # Use bfloat16 for RTX A4000
    optim="paged_adamw_8bit"
)
```

### ğŸ“Š Data Flow & Format

#### Input: Slack Export Structure
```
slack_export/
â”œâ”€â”€ users.json          # User metadata
â”œâ”€â”€ channels.json       # Channel metadata
â””â”€â”€ channel_name/
    â”œâ”€â”€ 2024-09-24.json  # Daily message files
    â””â”€â”€ 2024-09-25.json
```

#### Processing Pipeline
```
Slack JSON â†’ Thread Analysis â†’ Context Extraction â†’ Alpaca Format â†’ Training Dataset
```

#### Output: Training Dataset Format
```json
{
  "instruction": "ä»¥ä¸‹ã®å¯¾è©±ã®æ–‡è„ˆã«ç¶šã„ã¦ã€ã‚ãªãŸã‚‰ã—ãè¿”ä¿¡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚",
  "input": "ãƒ¦ãƒ¼ã‚¶ãƒ¼A: ã€conversation context from previous messagesã€‘",
  "output": "ã€target user's responseã€‘"
}
```

### ğŸ–¥ï¸ Environment Contexts

#### Development Environment (Laptop - CPU)
- **Purpose**: Code development, data processing, testing
- **Limitations**: No GPU training
- **Capabilities**: All data processing, small model inference

#### Training Environment (Workstation - GPU)
- **Hardware**: RTX A4000 (16GB VRAM)
- **Purpose**: LoRA fine-tuning, model inference
- **Optimizations**: Memory-efficient quantization, optimized batch sizes

### ğŸ”„ Key Workflows

#### 1. Data Preparation
```python
# Interactive data processing
from slack_data_processor import SlackDataProcessor
processor = SlackDataProcessor("path/to/slack/export")
result = processor.process("./processed_datasets")

# Notebook integration
process_slack_data_interactive()  # GUI-style selection
```

#### 2. Training Execution
```python
# Environment setup
python setup_env.py check

# Jupyter execution
jupyter lab
# Select "LoRA Persona (uv)" kernel
# Run cells sequentially
```

#### 3. Model Management
```python
# Built-in utilities
list_saved_models()
cleanup_old_models()
show_gpu_memory()
interactive_test()  # Real-time model testing
```

### ğŸ¯ Code Patterns and Conventions

#### Error Handling Pattern
```python
try:
    # Main operation
    result = operation()
    print(f"âœ… Success: {result}")
except Exception as e:
    print(f"âŒ Error: {e}")
    print("ğŸ’¡ Suggestion: [helpful guidance]")
```

#### Progress Indication Pattern
```python
print("Phase X: Starting [operation]...")
# Operation code
print(f"âœ… [Operation] completed. [Details]")
```

#### Memory Management Pattern
```python
torch.cuda.empty_cache()
gc.collect()
print(f"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
```

### ğŸ“ File Organization Rules

#### Tracked Files (Git)
- Source code: `*.py`, `*.ipynb`
- Documentation: `*.md`
- Configuration: `requirements.txt`, `setup_env.py`

#### Ignored Files (Git)
- Personal data: `sampledata/`, `processed_datasets/`
- Models: `lora-persona-model-*`, `lora-output-*`
- Environment: `.venv/`
- Temporary: `*.tmp`, `*.cache`

### ğŸš¨ Important Considerations

#### Data Privacy
- **Never commit personal Slack data**
- All training data is local-only
- Gitignore protects sensitive files
- Dataset paths are configurable

#### Hardware Adaptation
```python
# Auto-detection pattern
if torch.cuda.is_available():
    # GPU optimizations
    device_map = "auto"
    batch_size = 4
else:
    # CPU fallback
    device_map = "cpu"
    batch_size = 1
```

#### Package Management
```bash
# Always use uv, not pip
uv pip install package_name
uv pip install -r requirements.txt
uv pip list
uv pip uninstall package_name
```

### ğŸ”§ Development Guidelines

#### When Adding New Features
1. **Environment First**: Update `requirements.txt` if needed
2. **Error Handling**: Include comprehensive error messages
3. **Documentation**: Update relevant `.md` files
4. **Testing**: Verify on both CPU and GPU environments
5. **Privacy**: Ensure no personal data exposure

#### When Debugging Issues
1. **Check Environment**: `python setup_env.py check`
2. **Verify Packages**: `uv pip list`
3. **GPU Status**: Check CUDA availability
4. **Data Paths**: Verify file locations and permissions
5. **Memory**: Monitor GPU/CPU memory usage

#### Code Style
- **Japanese Support**: UTF-8 encoding throughout
- **Path Handling**: Use `pathlib.Path` for cross-platform compatibility
- **Logging**: Informative progress messages with emojis
- **Configuration**: Make paths and parameters easily configurable

### ğŸ¯ Common Tasks for AI Assistance

#### Data Processing Tasks
- Convert Slack exports to training format
- Analyze user activity and conversation patterns
- Handle various Slack export formats
- Quality filtering and data validation

#### Training Tasks
- Optimize LoRA parameters for different hardware
- Implement memory-efficient training strategies
- Add new model architectures or quantization methods
- Improve training monitoring and logging

#### Environment Tasks
- Package management and dependency resolution
- Cross-platform compatibility improvements
- GPU/CPU detection and optimization
- Development workflow automation

#### Documentation Tasks
- Update setup instructions
- Create troubleshooting guides
- Document new features and configurations
- Maintain compatibility information

This context should help AI assistants understand the project's purpose, architecture, and development patterns for effective assistance.