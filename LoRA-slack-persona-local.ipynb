{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05422a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slack data processor imported successfully!\n",
      "Available functions:\n",
      "1. process_slack_data_interactive() - Interactive processing of data in sampledata/slack/\n",
      "2. process_custom_slack_path('path/to/slack/export') - Process custom path\n",
      "\n",
      "Run one of these functions to start processing your Slack data!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Slack Data Processing - Convert Export to Training Dataset\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path to import our processor\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import our custom Slack data processor\n",
    "try:\n",
    "    from slack_data_processor import SlackDataProcessor\n",
    "    print(\"Slack data processor imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing slack_data_processor: {e}\")\n",
    "    print(\"Make sure slack_data_processor.py is in the same directory as this notebook\")\n",
    "\n",
    "def process_slack_data_interactive():\n",
    "    \"\"\"\n",
    "    Interactive function to process Slack data for training\n",
    "    \"\"\"\n",
    "    print(\"=== Slack Data Processing for LoRA Training ===\\n\")\n",
    "    \n",
    "    # Check if sample data exists\n",
    "    sample_path = Path(\"./sampledata/slack\")\n",
    "    if sample_path.exists():\n",
    "        print(f\"Found sample data directory: {sample_path}\")\n",
    "        \n",
    "        # List available export directories\n",
    "        export_dirs = [d for d in sample_path.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if export_dirs:\n",
    "            print(\"\\nAvailable Slack export directories:\")\n",
    "            for i, export_dir in enumerate(export_dirs, 1):\n",
    "                print(f\"{i}. {export_dir.name}\")\n",
    "            \n",
    "            # Let user select directory\n",
    "            try:\n",
    "                choice = int(input(f\"\\nSelect export directory (1-{len(export_dirs)}): \")) - 1\n",
    "                if 0 <= choice < len(export_dirs):\n",
    "                    selected_dir = export_dirs[choice]\n",
    "                    print(f\"Selected: {selected_dir}\")\n",
    "                    \n",
    "                    # Process the data\n",
    "                    processor = SlackDataProcessor(str(selected_dir))\n",
    "                    result = processor.process(\"./processed_datasets\")\n",
    "                    \n",
    "                    if result:\n",
    "                        print(\"\\n🎉 Success! Your dataset is ready for training!\")\n",
    "                        print(f\"Load this CSV file in the training cell: {result['csv']}\")\n",
    "                        return result\n",
    "                    else:\n",
    "                        print(\"❌ Processing failed.\")\n",
    "                        return None\n",
    "                else:\n",
    "                    print(\"Invalid selection.\")\n",
    "                    return None\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"No export directories found in sample data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Sample data directory not found: {sample_path}\")\n",
    "        print(\"Please place your Slack export in the sampledata/slack/ directory\")\n",
    "        return None\n",
    "\n",
    "def process_custom_slack_path(slack_path: str):\n",
    "    \"\"\"\n",
    "    Process Slack data from a custom path\n",
    "    \"\"\"\n",
    "    if not os.path.exists(slack_path):\n",
    "        print(f\"Path does not exist: {slack_path}\")\n",
    "        return None\n",
    "    \n",
    "    processor = SlackDataProcessor(slack_path)\n",
    "    return processor.process(\"./processed_datasets\")\n",
    "\n",
    "# Example usage:\n",
    "print(\"Available functions:\")\n",
    "print(\"1. process_slack_data_interactive() - Interactive processing of data in sampledata/slack/\")\n",
    "print(\"2. process_custom_slack_path('path/to/slack/export') - Process custom path\")\n",
    "print(\"\\nRun one of these functions to start processing your Slack data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af58a0c",
   "metadata": {},
   "source": [
    "# LoRA Persona Model Training - Local RTX A4000 Version\n",
    "\n",
    "This notebook is optimized for running locally on RTX A4000 GPU. It builds upon the original Colab notebook but with modifications for local execution:\n",
    "\n",
    "- Optimized memory usage for RTX A4000 (16GB VRAM)\n",
    "- Local file paths and data handling\n",
    "- Better error handling and logging\n",
    "- CUDA optimizations for local GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc03896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Phase 0: Environment Setup for Local RTX A4000\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Phase 0: Setting up environment for local RTX A4000...\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. This notebook requires GPU acceleration.\")\n",
    "\n",
    "# Install required packages if not already installed\n",
    "try:\n",
    "    import transformers\n",
    "    import datasets\n",
    "    import peft\n",
    "    import accelerate\n",
    "    import bitsandbytes\n",
    "    import trl\n",
    "    print(\"All required packages are already installed.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Missing package: {e}\")\n",
    "    print(\"Please install missing packages using:\")\n",
    "    print(\"pip install transformers datasets peft accelerate bitsandbytes trl sentencepiece\")\n",
    "\n",
    "print(\"Environment setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Phase 1: Base Model and Tokenizer Setup (Optimized for RTX A4000)\n",
    "# ==============================================================================\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"\\nPhase 1: Preparing base model and tokenizer...\")\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Model selection - you can change this to other models\n",
    "# Options for better local performance:\n",
    "# - \"microsoft/DialoGPT-medium\" (English, smaller)\n",
    "# - \"rinna/japanese-gpt-neox-3.6b\" (Japanese, smaller)\n",
    "# - \"stabilityai/japanese-stablelm-instruct-gamma-7b\" (Japanese, larger but good quality)\n",
    "model_id = \"stabilityai/japanese-stablelm-instruct-gamma-7b\"\n",
    "\n",
    "# Optimized quantization config for RTX A4000\n",
    "# RTX A4000 has 16GB VRAM, so we can be less aggressive with quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for better performance on RTX A4000\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {model_id}\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Load model with optimized settings for local GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # Optimize for RTX A4000\n",
    "    low_cpu_mem_usage=True,      # Optimize CPU memory usage\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Base model and tokenizer setup completed.\")\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"GPU Memory Used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Phase 2: Training Data Preparation (Local File Support)\n",
    "# ==============================================================================\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nPhase 2: Preparing training data...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Data Loading Options - Choose one of the following methods:\n",
    "# ==============================================================================\n",
    "\n",
    "# Option 1: Load processed dataset from slack_data_processor.py\n",
    "# RECOMMENDED: Use this after running the Slack Data Processing cell above\n",
    "processed_datasets_dir = Path(\"./processed_datasets\")\n",
    "if processed_datasets_dir.exists():\n",
    "    # Find the most recent CSV file\n",
    "    csv_files = list(processed_datasets_dir.glob(\"slack_persona_*.csv\"))\n",
    "    if csv_files:\n",
    "        # Sort by modification time, get most recent\n",
    "        latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"Loading processed dataset: {latest_csv}\")\n",
    "        \n",
    "        df_loaded = pd.read_csv(latest_csv)\n",
    "        my_slack_data = df_loaded.to_dict('records')\n",
    "        print(f\"✅ Loaded {len(my_slack_data)} training examples from processed dataset\")\n",
    "        \n",
    "        # Show sample of loaded data\n",
    "        print(\"\\n--- Sample from loaded dataset ---\")\n",
    "        print(f\"Input: {my_slack_data[0]['input'][:100]}...\")\n",
    "        print(f\"Output: {my_slack_data[0]['output'][:100]}...\")\n",
    "        print(\"----------------------------------\")\n",
    "    else:\n",
    "        print(\"No processed CSV files found. Using sample data instead.\")\n",
    "        print(\"💡 Run the Slack Data Processing cell first to create your dataset!\")\n",
    "        csv_files = None\n",
    "else:\n",
    "    print(\"Processed datasets directory not found. Using sample data.\")\n",
    "    csv_files = None\n",
    "\n",
    "# Option 2: Load from custom JSON/CSV file\n",
    "# Uncomment and modify the path below if you have a custom file\n",
    "# custom_file = Path(\"./path/to/your/custom_data.csv\")  # or .json\n",
    "# if custom_file.exists():\n",
    "#     if custom_file.suffix == '.csv':\n",
    "#         df_loaded = pd.read_csv(custom_file)\n",
    "#         my_slack_data = df_loaded.to_dict('records')\n",
    "#     else:  # JSON\n",
    "#         with open(custom_file, 'r', encoding='utf-8') as f:\n",
    "#             my_slack_data = json.load(f)\n",
    "#     print(f\"Loaded {len(my_slack_data)} records from {custom_file}\")\n",
    "#     csv_files = [custom_file]  # Mark as loaded\n",
    "\n",
    "# Option 3: Sample data (fallback)\n",
    "# This will be used if no processed data is found\n",
    "if not csv_files:\n",
    "    print(\"⚠️  Using sample data. For better results, process your actual Slack data first!\")\n",
    "    my_slack_data = [\n",
    "    {\n",
    "        \"instruction\": \"以下の対話の文脈に続いて、あなたらしく返信を生成してください。\",\n",
    "        \"input\": \"ユーザーA: 「来週の定例会議ですが、アジェンダ案を共有します。何か追加項目はありますか？」\",\n",
    "        \"output\": \"ありがとうございます！拝見しました。可能であれば、先日話していた新機能の進捗についても5分ほど時間をいただけると嬉しいです。\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"以下の対話の文脈に続いて、あなたらしく返信を生成してください。\",\n",
    "        \"input\": \"ユーザーB: 「この前の件、調査してみたんですが、原因はサーバー側の設定ミスだったみたいです。すみません…。」\",\n",
    "        \"output\": \"調査ありがとうございます！原因が特定できてよかったです。誰にでもミスはありますし、気にしないでください。すぐに対応してくれて助かりました！\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"以下の対話の文脈に続いて、あなたらしく返信を生成してください。\",\n",
    "        \"input\": \"ユーザーC: 「新しいAIの論文読んだ？すごく面白かったよ。」\",\n",
    "        \"output\": \"読みました！特にエージェントが自律的に協調する部分、示唆に富んでますよね。あれを私たちのプロダクトに応用できないか、少し考えてみたくなります。\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"以下の対話の文脈に続いて、あなたらしく返信を生成してください。\",\n",
    "        \"input\": \"ユーザーD: 「今度のプロジェクト、スケジュールがタイトですね。どう思います？」\",\n",
    "        \"output\": \"確かにタイトですね。まずは最重要な機能を特定して、段階的にリリースする方針はどうでしょうか？品質を保ちつつ、現実的なスケジュールで進められると思います。\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"以下の対話の文脈に続いて、あなたらしく返信を生成してください。\",\n",
    "        \"input\": \"ユーザーE: 「お疲れ様です。今日のデモ、とても良かったです！」\",\n",
    "        \"output\": \"お疲れ様でした！ありがとうございます。チーム全員で準備した甲斐がありました。特にどの部分が印象的でしたか？今後の参考にさせていただきたいです。\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame and display sample\n",
    "df = pd.DataFrame(my_slack_data)\n",
    "print(\"\\n--- Training Data Sample ---\")\n",
    "print(df.head())\n",
    "print(\"----------------------------\")\n",
    "print(f\"Total data records: {len(df)}\")\n",
    "\n",
    "if len(df) < 50:\n",
    "    print(\"\\nWARNING: You have very few training examples. For better results, consider adding more data.\")\n",
    "    print(\"Recommended: 100+ examples for decent results, 300+ for good results.\")\n",
    "\n",
    "# Create prompt formatting function\n",
    "def create_prompt(data_point):\n",
    "    prompt = f\"\"\"以下は、タスクを説明する指示と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### 入力:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### 応答:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(create_prompt)\n",
    "\n",
    "print(\"\\nTraining data preparation completed.\")\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "\n",
    "# Save processed data locally for future use\n",
    "output_dir = Path(\"./training_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "dataset.save_to_disk(str(output_dir / \"processed_dataset\"))\n",
    "print(f\"Processed dataset saved to: {output_dir / 'processed_dataset'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6de4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Phase 3: LoRA Fine-tuning (Optimized for RTX A4000)\n",
    "# ==============================================================================\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\nPhase 3: Starting LoRA fine-tuning...\")\n",
    "\n",
    "# Clear GPU memory before training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# LoRA configuration optimized for RTX A4000\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank - balance between capability and memory usage\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"  # Additional modules for better coverage\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"./lora-output-{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Training arguments optimized for RTX A4000\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,  # Start with 3 epochs, adjust based on results\n",
    "    per_device_train_batch_size=4,  # RTX A4000 can handle larger batches\n",
    "    gradient_accumulation_steps=2,   # Effective batch size = 4 * 2 = 8\n",
    "    optim=\"paged_adamw_8bit\",        # Memory-efficient optimizer\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,  # RTX A4000 supports bfloat16 better\n",
    "    bf16=True,   # Use bfloat16 for better numerical stability\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=50,\n",
    "    evaluation_strategy=\"no\",  # Disable evaluation to save memory\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    dataloader_num_workers=0,  # Reduce CPU usage\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for local training\n",
    "    run_name=f\"lora-persona-{timestamp}\",\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=1024,  # Adjust based on your data\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,  # Disable packing for better control\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n=== Training Started ===\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: {len(dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "# Monitor GPU memory before training\n",
    "print(f\"GPU Memory before training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n=== Training Completed ===\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = f\"./lora-persona-model-{timestamp}\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Trained LoRA adapter saved to: {model_save_path}\")\n",
    "print(f\"Training logs saved to: {output_dir}/logs\")\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    \"model_id\": model_id,\n",
    "    \"dataset_size\": len(dataset),\n",
    "    \"num_epochs\": training_args.num_train_epochs,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"lora_rank\": lora_config.r,\n",
    "    \"lora_alpha\": lora_config.lora_alpha,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"save_path\": model_save_path\n",
    "}\n",
    "\n",
    "with open(f\"{model_save_path}/training_info.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Training information saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2803e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Phase 4: Inference Testing (Local Optimized)\n",
    "# ==============================================================================\n",
    "\n",
    "from peft import PeftModel\n",
    "import time\n",
    "\n",
    "print(\"\\nPhase 4: Testing the trained persona model...\")\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load base model for inference\n",
    "print(\"Loading base model for inference...\")\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the trained LoRA adapter\n",
    "print(f\"Loading trained LoRA adapter from: {model_save_path}\")\n",
    "inference_model = PeftModel.from_pretrained(inference_model, model_save_path)\n",
    "inference_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully for inference.\")\n",
    "\n",
    "# Response generation function with improved parameters\n",
    "def generate_response(instruction, context, max_new_tokens=256, temperature=0.7):\n",
    "    prompt = f\"\"\"以下は、タスクを説明する指示と、さらなるコンテキストを提供する入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示:\n",
    "{instruction}\n",
    "\n",
    "### 入力:\n",
    "{context}\n",
    "\n",
    "### 応答:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=768)\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.05,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # Decode and extract response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the response part\n",
    "    if \"### 応答:\" in generated_text:\n",
    "        response = generated_text.split(\"### 応答:\")[1].strip()\n",
    "    else:\n",
    "        response = \"[応答の抽出に失敗しました]\"\n",
    "    \n",
    "    return response, generation_time\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"instruction\": \"以下の対話の文脈に続いて、あなたらしく返信を生成してください。\",\n",
    "        \"context\": \"ユーザーF: 「急で申し訳ないんだけど、この資料のレビューお願いできないかな？ 明日の朝までだと嬉しいんだけど…。」\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"以下の対話の文脈に続いて、あなたらしく返信を生成してください。\",\n",
    "        \"context\": \"ユーザーG: 「新しい技術について調べているんですが、どう思いますか？\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"以下の対話の文脈に続いて、あなたらしく返信を生成してください。\",\n",
    "        \"context\": \"ユーザーH: 「今度のプレゼン、緊張するなあ。何かアドバイスありますか？\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n=== Inference Testing ===\")\n",
    "total_time = 0\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n--- Test Case {i} ---\")\n",
    "    print(f\"Context: {test_case['context']}\")\n",
    "    \n",
    "    response, gen_time = generate_response(\n",
    "        test_case[\"instruction\"], \n",
    "        test_case[\"context\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated Response: {response}\")\n",
    "    print(f\"Generation time: {gen_time:.2f} seconds\")\n",
    "    total_time += gen_time\n",
    "\n",
    "print(f\"\\n=== Testing Completed ===\")\n",
    "print(f\"Average generation time: {total_time / len(test_cases):.2f} seconds\")\n",
    "print(f\"GPU Memory Used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Interactive testing function\n",
    "def interactive_test():\n",
    "    print(\"\\n=== Interactive Testing Mode ===\")\n",
    "    print(\"Enter 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter context for testing: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        instruction = \"以下の対話の文脈に続いて、あなたらしく返信を生成してください。\"\n",
    "        response, gen_time = generate_response(instruction, user_input)\n",
    "        print(f\"\\nResponse: {response}\")\n",
    "        print(f\"Time: {gen_time:.2f}s\")\n",
    "\n",
    "print(\"\\nYou can now test your model interactively by calling: interactive_test()\")\n",
    "print(\"All training and inference setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Bonus: Model Management and Utilities\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def list_saved_models():\n",
    "    \"\"\"List all saved LoRA models in the current directory\"\"\"\n",
    "    models = []\n",
    "    for item in Path(\".\").iterdir():\n",
    "        if item.is_dir() and \"lora-persona-model\" in item.name:\n",
    "            models.append(item)\n",
    "    \n",
    "    print(\"Saved LoRA models:\")\n",
    "    for i, model in enumerate(models, 1):\n",
    "        print(f\"{i}. {model.name}\")\n",
    "        # Try to read training info if available\n",
    "        info_file = model / \"training_info.json\"\n",
    "        if info_file.exists():\n",
    "            with open(info_file, 'r') as f:\n",
    "                info = json.load(f)\n",
    "                print(f\"   - Dataset size: {info.get('dataset_size', 'Unknown')}\")\n",
    "                print(f\"   - Epochs: {info.get('num_epochs', 'Unknown')}\")\n",
    "                print(f\"   - Learning rate: {info.get('learning_rate', 'Unknown')}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def cleanup_old_models(keep_latest=2):\n",
    "    \"\"\"Clean up old model directories, keeping only the latest N models\"\"\"\n",
    "    models = list_saved_models()\n",
    "    if len(models) <= keep_latest:\n",
    "        print(f\"Only {len(models)} models found, no cleanup needed.\")\n",
    "        return\n",
    "    \n",
    "    # Sort by modification time\n",
    "    models.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    models_to_delete = models[keep_latest:]\n",
    "    print(f\"Deleting {len(models_to_delete)} old models...\")\n",
    "    \n",
    "    for model in models_to_delete:\n",
    "        print(f\"Deleting: {model.name}\")\n",
    "        shutil.rmtree(model)\n",
    "    \n",
    "    print(\"Cleanup completed.\")\n",
    "\n",
    "def show_gpu_memory():\n",
    "    \"\"\"Display current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        print(f\"GPU Memory Status:\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved:  {reserved:.2f} GB\")\n",
    "        print(f\"  Total:     {total:.2f} GB\")\n",
    "        print(f\"  Free:      {total - reserved:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache to free up memory\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"GPU cache cleared.\")\n",
    "    show_gpu_memory()\n",
    "\n",
    "# Show available utilities\n",
    "print(\"Available utility functions:\")\n",
    "print(\"- list_saved_models(): List all saved LoRA models\")\n",
    "print(\"- cleanup_old_models(keep_latest=2): Clean up old models\")\n",
    "print(\"- show_gpu_memory(): Show current GPU memory usage\")\n",
    "print(\"- clear_gpu_cache(): Clear GPU cache\")\n",
    "print(\"- interactive_test(): Start interactive testing mode\")\n",
    "\n",
    "# Show current status\n",
    "print(\"\\nCurrent status:\")\n",
    "show_gpu_memory()\n",
    "list_saved_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lora-persona (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
